{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2acb94-ed1a-4e32-9856-27ca865a7527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Library/Python/3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_checkpoint)\n\u001b[0;32m---> 11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_checkpoint)\n\u001b[1;32m     12\u001b[0m new_words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<vertex>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<r>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<t>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<h>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[[\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<end>\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39madd_tokens(new_words)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py:1475\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1475\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py:1463\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1461\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import pandas\n",
    "\n",
    "model_checkpoint = \"t5-small\"\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n",
    "new_words = ['<vertex>', '<r>', '<t>', '<h>', '[[', ']]', '<end>']\n",
    "tokenizer.add_tokens(new_words)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# get the dataset\n",
    "with open('DR.pkl', 'rb') as file:\n",
    "  data_dump = pickle.load(file)\n",
    "\n",
    "train_datadict = {'document': [], 'summary': []}\n",
    "validate_datadict = {'document': [], 'summary': []}\n",
    "for x, data_point in enumerate(data_dump):\n",
    "  if x < 2453:\n",
    "    train_datadict['document'].append(data_point['text'])\n",
    "    train_datadict['summary'].append(data_point['linearized'])\n",
    "  else:\n",
    "    validate_datadict['document'].append(data_point['text'])\n",
    "    validate_datadict['summary'].append(data_point['linearized'])\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_datadict)\n",
    "validate_dataset = Dataset.from_dict(validate_datadict)\n",
    "dataset = DatasetDict({'train': train_dataset, 'validation': validate_dataset})\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    model_inputs = tokenizer(examples['document'], max_length = 2048, truncation = True, padding = True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        targets = tokenizer(examples['summary'], max_length = 1024, truncation = True, padding = True)\n",
    "    model_inputs['labels'] = targets['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_data, batched = True, remove_columns=['document','summary'])\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model,return_tensors='pt')\n",
    "batch_size = 8\n",
    "train_data_loader = DataLoader(tokenized_datasets[\"train\"], shuffle = True, batch_size = batch_size, collate_fn = data_collator)\n",
    "eval_data_loader = DataLoader(tokenized_datasets[\"validation\"], shuffle = True, batch_size = batch_size, collate_fn = data_collator)\n",
    "\n",
    "def _delinearize_relations(strings):\n",
    "    output = []\n",
    "    for string in strings:\n",
    "        ht = string.split('<h>')\n",
    "        relation_type = ht.pop(0).replace(' ', '')\n",
    "        final_split = ht[0].split('<t>')\n",
    "        head = int(final_split.pop(0))\n",
    "        if '<end>' in final_split[0]:\n",
    "            # final_split[0] = final_split[0].replace('<pad>', '')\n",
    "            tail = int(final_split.pop()[:-11])\n",
    "        else:\n",
    "            tail = int(final_split.pop())\n",
    "        output.append({'r': relation_type, 'h': head, 't': tail})\n",
    "    return output\n",
    "\n",
    "# takes a list of linear strings, and a list with the corresponding article input\n",
    "# returns a list of dictionary items included in linear string\n",
    "\n",
    "# this is going to update the inputs dataset by filling in the veritices and relations frield for each data_point\n",
    "def delinearize(linear_strings, dataset):\n",
    "    for x, string in enumerate(linear_strings):\n",
    "        string = string.replace('<pad>', '')\n",
    "        split = string.split('<r>')\n",
    "        vertices = split.pop(0)\n",
    "        relations = _delinearize_relations(split)\n",
    "        vertices = vertices.split('<vertex>')\n",
    "        vertices.pop(0)\n",
    "        vertex_data_form = dict()\n",
    "        for vertex in vertices:\n",
    "            split = vertex.split('[[')\n",
    "            if '</s>' in split[1]:\n",
    "                split[1] = split[1].replace('</s>', '')\n",
    "                split[1] = split[1].replace('<end> ', '')\n",
    "            vertex_data_form[int(split[1][:-3])] = split[0][1:-1]\n",
    "        dataset[x]['relations'] = relations\n",
    "        dataset[x]['vertexList'] = vertex_data_form\n",
    "        dataset[x]['linearized'] = string\n",
    "\n",
    "# helper function\n",
    "# return an array of relations with the same head and tail vertices\n",
    "# if none exist return None type\n",
    "def _match(relation, comparisons):\n",
    "    output = []\n",
    "    print('comparisons: ', comparisons)\n",
    "    for comparison in comparisons:\n",
    "        print('comparison: ', comparison)\n",
    "        print('relation: ', relation)\n",
    "        if comparison['h'] == relation['h'] and comparison['t'] == relation['t']:\n",
    "          if comparison['r'] == relation['r']:\n",
    "            return comparison\n",
    "          else:\n",
    "            output.append(comparison)\n",
    "    if output == []:\n",
    "        return None\n",
    "    else:\n",
    "        return output\n",
    "\n",
    "\n",
    "# confusion matrix generator\n",
    "def generate_confusion_matrix(pred_labels, true_labels, possible_labels):\n",
    "  matrix = [[0 for x in range(len(possible_labels))] for y in range(len(possible_labels))]\n",
    "  matched_full = []\n",
    "  print()\n",
    "  for x, article in enumerate(pred_labels):\n",
    "    for relation in article:\n",
    "      print(relation)\n",
    "      matches = _match(relation, true_labels[x])\n",
    "      pred_idx = possible_labels.index(relation['r'])\n",
    "      if matches is None:\n",
    "        # generated an invalid relation\n",
    "        matrix[pred_idx][-1] += 1\n",
    "        matched_full.append(relation)\n",
    "      elif matches is list:\n",
    "        # we generated the right pair worng relation ID\n",
    "        actual_idx = [possible_labels.index(matches[0]['r'])]\n",
    "        matrix[pred_idx][actual_idx] += 1\n",
    "        matched_full.append(relation[0])\n",
    "      else:\n",
    "        actual_idx = [possible_labels.index(matches['r'])]\n",
    "        matrix[pred_idx][actual_idx] += 1\n",
    "        matched_full.append(relation)\n",
    "    if not matched_full == true_labels[x]:\n",
    "      for x, label in enumerate(true_labels[x]):\n",
    "        if label not in matched_full:\n",
    "          matrix[-1][possible_labels.index(label['r'].replace(' ', ''))] += 1\n",
    "\n",
    "  return matrix\n",
    "\n",
    "# get the valid labels\n",
    "def get_labels(filepath):\n",
    "  with open(filepath, 'r') as file:\n",
    "    table = pandas.read_json(file, typ='series')\n",
    "  keys = [key for key in table.keys()]\n",
    "  return keys\n",
    "\n",
    "\n",
    "def compute_accuracy(eval_pred):\n",
    "  predictions, labels = eval_pred\n",
    "  token_preds = tokenizer.batch_decode(predictions)\n",
    "  token_labs = tokenizer.batch_decode(labels)\n",
    "\n",
    "  #need to delinearize before confusion matrix\n",
    "  # need dictionaries for preds and labs of each data_point\n",
    "  pred_dicts = [{'linearized': pred_lin} for pred_lin in token_preds]\n",
    "  lab_dicts = [{'linearized': lab_lin} for lab_lin in token_labs]\n",
    "\n",
    "  delinearize(token_preds, pred_dicts)\n",
    "  delinearize(token_labs, lab_dicts)\n",
    "\n",
    "  pred_rels = [article['relations'] for article in pred_dicts]\n",
    "  lab_rels = [article['relations'] for article in lab_dicts]\n",
    "\n",
    "\n",
    "  label_list = get_labels('rel_info.json')\n",
    "  confusion_matrix = generate_confusion_matrix(pred_rels, lab_rels, label_list)\n",
    "\n",
    "\n",
    "  num_correct = 0\n",
    "  for x in range(len(label_list)):\n",
    "    num_correct += confusion_matrix[x][x]\n",
    "\n",
    "  return {'accuracy': num_correct/len(predictions)}\n",
    "\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{model_checkpoint}-DocRED\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_accuracy,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc937c9a-55d9-458f-8374-902c578b3efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
