# This is where our human-readable evaluation metrics will go!
# e.g. precision, recall, f1, etc.

def compute_score(true_rels, pred_rels):
    return { 'f1': 0, 'precision': 0, 'recall': 0 }